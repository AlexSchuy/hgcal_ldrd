{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, time, os.path as osp, logging, numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt, cycler\n",
    "import matplotlib\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGCAL_LDRD_PATH = osp.abspath('../../hgcal_ldrd/src')\n",
    "PVCNN_PATH = osp.abspath('../../pvcnn')\n",
    "sys.path.append(HGCAL_LDRD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGCAL_SCRIPTS_PATH = osp.abspath('../../hgcal_ldrd')\n",
    "sys.path.append(HGCAL_SCRIPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.hitgraphs import HitGraphDataset\n",
    "sys.path.append(PVCNN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import pvcnn_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = pvcnn_script.TrainingScript(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\u001b[33m    INFO:2020-05-19 21:44:34:pvcnn_script:95\u001b[0m Using dataset_path /home/alexjschuy/hgcal_ldrd/scripts/../../data/single-tau\n\u001b[33m    INFO:2020-05-19 21:44:34:pvcnn_script:110\u001b[0m 10000, [ 8000  8000 10000]\n\u001b[33m    INFO:2020-05-19 21:44:34:pvcnn_script:152\u001b[0m using device cuda\n\u001b[33m    INFO:2020-05-19 21:44:34:base:31\u001b[0m Model: \nPVConvForHGCAL(\n  (point_features): ModuleList(\n    (0): PVConv(\n      (voxelization): Voxelization(resolution=32, normalized eps = 0)\n      (voxel_layers): Sequential(\n        (0): Conv3d(5, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (1): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (4): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n      (point_features): SharedMLP(\n        (layers): Sequential(\n          (0): Conv1d(5, 64, kernel_size=(1,), stride=(1,))\n          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n    )\n    (1): PVConv(\n      (voxelization): Voxelization(resolution=16, normalized eps = 0)\n      (voxel_layers): Sequential(\n        (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (1): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (4): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n      (point_features): SharedMLP(\n        (layers): Sequential(\n          (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n    )\n    (2): PVConv(\n      (voxelization): Voxelization(resolution=16, normalized eps = 0)\n      (voxel_layers): Sequential(\n        (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (1): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (4): BatchNorm3d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n      (point_features): SharedMLP(\n        (layers): Sequential(\n          (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n    )\n    (3): PVConv(\n      (voxelization): Voxelization(resolution=16, normalized eps = 0)\n      (voxel_layers): Sequential(\n        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (1): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n        (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n        (4): BatchNorm3d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n      (point_features): SharedMLP(\n        (layers): Sequential(\n          (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n      )\n    )\n    (4): SharedMLP(\n      (layers): Sequential(\n        (0): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n  )\n  (cloud_features): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=1024, out_features=256, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Linear(in_features=256, out_features=128, bias=True)\n      (1): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): SharedMLP(\n      (layers): Sequential(\n        (0): Conv1d(1472, 512, kernel_size=(1,), stride=(1,))\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (1): Dropout(p=0.3, inplace=False)\n    (2): SharedMLP(\n      (layers): Sequential(\n        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Conv1d(256, 4, kernel_size=(1,), stride=(1,))\n  )\n)\nParameters: 2562244\n\u001b[33m WARNING:2020-05-19 21:44:34:pvcnn_script:195\u001b[0m Loading weights from previous checkpoint: ../output/checkpoints/model_checkpoint_PVConvForHGCAL_2562244_9c8b11eb88_alexjschuy.best.pth.tar\n"
    }
   ],
   "source": [
    "script.load_checkpoint = '../output/checkpoints/model_checkpoint_PVConvForHGCAL_2562244_9c8b11eb88_alexjschuy.best.pth.tar'\n",
    "full_dataset, train_dataset, valid_dataset = script.get_full_dataset()\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=script.valid_batch_size, shuffle=False)\n",
    "trainer = script.get_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (0) must match the existing size (20038) at non-singleton dimension 0.  Target sizes: [0].  Tensor sizes: [20038]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-13bc33e4f2a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pvcnn/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hgcal_ldrd/src/training/gnn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (0) must match the existing size (20038) at non-singleton dimension 0.  Target sizes: [0].  Tensor sizes: [20038]"
     ]
    }
   ],
   "source": [
    "output_dir = Path('../../data/single-tau/output')\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "for i, d in enumerate(valid_dataset):\n",
    "    one_file_subset = torch.utils.data.Subset(valid_dataset,[i])\n",
    "    one_file_loader = DataLoader(one_file_subset, batch_size=1, shuffle=False)\n",
    "    predictions = trainer.predict(one_file_loader)\n",
    "    input = one_file_subset[0].x.cpu().numpy()\n",
    "    output = np.zeros((input.shape[0], input.shape[1]+1))\n",
    "    output[:, :-1] = input\n",
    "    output[:, -1] = predictions\n",
    "    output_path = output_dir / f'data_{i}.npy'\n",
    "    np.save(output_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}